# vLLM server config for Nemotron-Nano-12B-v2 on RTX 5090
# This file is loaded by `vllm serve --config /workspace/config.yaml`

# ---- Model & server ----
model: "nvidia/NVIDIA-Nemotron-Nano-12B-v2"
trust-remote-code: true
host: "0.0.0.0"
port: 8000

# ---- Nemotron / Blackwell specifics ----
# IMPORTANT for Nemotron v2 quality with Mamba-SSM layers
mamba-ssm-cache-dtype: "float32"

# Max context length (prompt + generated). 131072 == 128K tokens.
max-model-len: 131072

# Concurrency; tune based on VRAM. (64 is fine to start; lower if you OOM.)
max-num-seqs: 32

# ---- Defaults source + overrides ----
# Use vLLM's built-ins, then replace with our overrides below.
generation-config: "vllm"

# vLLM expects a JSON string here (not a YAML map).
# We set a one-shot, reasoning-friendly default; clients can still override.
override-generation-config: >
  {
    "temperature": 0.6,
    "top_p": 0.95,
    "top_k": 50,
    "repetition_penalty": 1.0,
    "presence_penalty": 0.0,
    "frequency_penalty": 0.0,
    "max_new_tokens": 131072
  }
