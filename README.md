# nemotron_vllm_autoconfig

> Production-ready vLLM â€œjust worksâ€ setup for **NVIDIA Nemotronâ€‘Nanoâ€‘12Bâ€‘v2** on **UbuntuÂ 24.04 LTS + RTXÂ 5090** â€” tuned for my personal server and exact hardware.  
> Repo link: https://github.com/BigBIueWhale/personal_server

---

ðŸ”´ðŸ”´ðŸ”´

# âš ï¸ **MODEL QUALITY DISCLAIMER**

**NVIDIAâ€‘Nemotronâ€‘Nanoâ€‘12Bâ€‘v2 performs poorly at coding tasks.** For context, even the **older *****Mistral Codestralâ€‘22B (original, MayÂ 2024)***â€”practically *ancient* in LLM termsâ€”still beats Nemotron at code. **This is an example comparison, not a recommendation to use Codestral.**

This repo exists to demonstrate a vLLM server configuration for Nemotron, **not** to recommend it for code work. If you care about code quality, choose a different model that fits your needs.

ðŸ”´ðŸ”´ðŸ”´

---

## What this is

This project pins **serverâ€‘side defaults** for vLLM so that **OpenWebUI** (or any OpenAIâ€‘compatible client) doesnâ€™t have to pass a dozen sampling knobs. Itâ€™s built for **my specific machine**:
- **GPU:** RTXÂ 5090 (Blackwell, sm_120)
- **Driver:** 580.xx (open)  
- **CUDA:** 13.0
- **OS:** UbuntuÂ 24.04 LTS

With this exact combo, **Nemotronâ€‘Nanoâ€‘12Bâ€‘v2** runs **in full precision (no quantization)** and stays stable. We intentionally use **vLLM** (not Ollama here) because vLLM lets us hardâ€‘code *all* decoding defaults on the server.

> âš ï¸ **Why custom config is needed (philosophy):** Pretty much all models **require very specific runtime flags** to behave correctly. For Nemotron v2, using the wrong defaults can make the model **quietly underperform (â€œsilently stupidâ€)** without obvious errors. This repo bakes in the correct settings â€” notably `mamba-ssm-cache-dtype: float32` â€” plus conservative decoding defaults. Clients can still tweak a couple of knobs (e.g., `temperature`), but the server remains the single source of truth for everything else.

---

## Contents

- **[`config.yaml`](./config.yaml)** â€” vLLM server config. Sets the model, networking, Nemotronâ€‘specific flags, and **complete** default decoding parameters (temperature/topâ€‘p/topâ€‘k/penalties/max_tokens, etc.). Clients inherit these unless they explicitly override a field.
- **[`run_vllm.sh`](./run_vllm.sh)** â€” oneâ€‘shot â€œinstallâ€ script that pulls the NVIDIA vLLM container and creates a persistent Docker container named `nemotron_vllm` with `--restart unless-stopped`. Run it once; use `docker start nemotron_vllm` on reboots.
- **[`OPENWEBUI_SETUP.md`](./OPENWEBUI_SETUP.md)** â€” stepâ€‘byâ€‘step wiring for OpenWebUI (OpenAIâ€‘compatible backend), written in the same style as my earlier notes.
- **[`UNINSTALL.md`](./UNINSTALL.md)** â€” how to stop/remove the container and (optionally) delete the Docker image and HF cache.

![placeholder](./docs/nemotron_12b_v2_benchmarks_versus_qwen3_32b.jpeg)

---

## Quick start

1. **Optional:** export your HF token (for firstâ€‘time download from Hugging Face):
   ```bash
   export HF_TOKEN=hf_************************
   ```

2. **Install / create the server container** (one time):
   ```bash
   chmod +x ./run_vllm.sh
   ./run_vllm.sh
   ```

3. **Test the endpoint:**
   ```bash
   # From the host:
   curl -s http://172.17.0.1:8000/v1/models | jq .
   ```

4. **Hook up OpenWebUI:** see **[OPENWEBUI_SETUP.md](./OPENWEBUI_SETUP.md)**.

---

## Why these settings?

* **Nemotronâ€‘specific requirement:** `mamba-ssm-cache-dtype: float32` dramatically affects quality on Nemotronâ€‘Nanoâ€‘12Bâ€‘v2. This is enforced in `config.yaml` so you canâ€™t forget it.
* **Large context with realistic concurrency:** The config sets `max-model-len: 131072` (â‰ˆ128K tokens) and **`max-num-seqs: 1`**. With 12B params on an RTXÂ 5090 and fullâ€‘precision weights, this leaves ~4â€“5Â GiB for KV cache at startup, which practically limits safe concurrency to ~1 request at 128K context.
* **Decoding defaults:** We pin **temperatureÂ 0.6 / top_pÂ 0.95 / top_kÂ 50**, with neutral repetition/presence/frequency penalties and a generous `max_new_tokens` default (131072). Clients can override perâ€‘request; the server supplies sensible fallback behavior.
* The containerized vLLM server listens on `0.0.0.0:8000` **inside the container**, and the Docker publish in `run_vllm.sh` maps it to **`172.17.0.1:8000` on the host** (the Docker bridge gateway). This keeps the API reachable to containers (e.g., OpenWebUI via `host.docker.internal`) while not exposing it on your hostâ€™s primary interfaces. If you ever need LAN exposure, change `BIND` to `0.0.0.0` (see `OPENWEBUI_SETUP.md`).

### Notes about the NVIDIA vLLM image

* **Flashâ€‘Attention backend** is used automatically on Blackwell; no extra flags required.
* **Mamba engine log line:** Youâ€™ll see a log like â€œMamba is experimental on VLLM_USE_V1=1. Falling back to V0 Engine.â€ Thatâ€™s expected with this image/version and fine for Nemotronâ€‘v2.
* **Chunked prefill:** vLLM autoâ€‘enables it for contexts larger thanÂ 32K and will warn that it â€œmight not work with some features/models.â€ Leave it on unless you observe issues; then relaunch with `--enable-chunked-prefill=False`.

---

## Compatibility notes

- Uses **NVIDIA vLLM container** (`nvcr.io/nvidia/vllm:25.09-py3`) built for **CUDAÂ 13** and **Blackwell** (RTXÂ 50â€‘series). No wheel rebuilds, no mismatched compute capability issues.
- Assumes you already installed the **NVIDIA Container Toolkit** and your driver exposes the GPU inside containers via `--gpus all`.

---

## Operating the server (start / stop / pause)

### Check status

```bash
docker ps
curl -s http://172.17.0.1:8000/v1/models | jq .
docker logs -f nemotron_vllm
```

### Stop (frees VRAM)

> Use **stop** to fully release GPU memory.

```bash
docker stop nemotron_vllm
```

Verify with:

```bash
nvidia-smi
```

### Start (loads model and uses VRAM again)

```bash
docker start -a nemotron_vllm   # attach logs
# or
docker start nemotron_vllm
```

### Restart

```bash
docker restart nemotron_vllm
```

### Pause vs Stop

* `docker pause` **does not free VRAM** (the process is frozen but GPU memory stays allocated).
* `docker stop` **does free VRAM** (the process exits and releases the GPU).

```bash
docker pause nemotron_vllm
docker unpause nemotron_vllm
```

### Auto-restart policy

The container is created with `--restart unless-stopped`.

* Disable auto-restart:

```bash
docker update --restart=no nemotron_vllm
```

* Re-enable:

```bash
docker update --restart=unless-stopped nemotron_vllm
```

### Remove and recreate (if you want a clean slate)

```bash
docker stop nemotron_vllm || true
docker rm nemotron_vllm || true
./run_vllm.sh
```

## License

This repo contains only configuration and scripts I wrote for my own server layout. Model weights are **not** distributed here; they are fetched from Hugging Face under their license terms.
